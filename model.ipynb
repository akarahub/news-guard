{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoConfig\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# YAML config\n",
    "try:\n",
    "    with open(r\".\\config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "except Exception as e:\n",
    "    raise\n",
    "\n",
    "# Logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(funcName)s - %(message)s\",\n",
    "    filename=config[\"log_dir\"] +\n",
    "    f\"{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}.log\",\n",
    "    filemode=\"w\"\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(\"Config file and logger setup completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_excel(config[\"data_path\"], index_col=0)\n",
    "except FileNotFoundError:\n",
    "    logger.error(\"File not found.  Ensure it is in the correct location.\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error reading Excel file: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data analysis\n",
    "null_count = df.isnull().sum()\n",
    "logger.info(f\"Null count: {null_count}\")\n",
    "\n",
    "df_copy = df.copy().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution\n",
    "df_copy[\"label\"].value_counts(ascending=True).plot.bar(color=[\"green\", \"blue\"])\n",
    "plt.title(\"Label Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average tokens per word\n",
    "np.mean(df_copy[\"title\"].str.split().apply(len)*1.5)\n",
    "\n",
    "df_copy[\"title_tokens\"] = df_copy[\"title\"].str.split().apply(len)*1.5\n",
    "df_copy[\"text_tokens\"] = df_copy[\"text\"].str.split().apply(len)*1.5\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "ax[0].hist(df_copy[\"title_tokens\"], bins=50, color=\"green\")\n",
    "ax[1].hist(df_copy[\"text_tokens\"], bins=50, color=\"blue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Process\n",
    "train, test = train_test_split(\n",
    "    df_copy, test_size=0.3, stratify=df_copy[\"label\"])\n",
    "test, validation = train_test_split(\n",
    "    test, test_size=1/3, stratify=test[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_pandas(train, preserve_index=False),\n",
    "        \"test\": Dataset.from_pandas(test, preserve_index=False),\n",
    "        \"validation\": Dataset.from_pandas(validation, preserve_index=False)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics_evaluate(eval_pred):\n",
    "    \"\"\"Evaluate metrics\"\"\"\n",
    "    try:\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        return accuracy.compute(predictions=predictions, references=labels)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while evaluate: {e}\")\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Return accuracy and F1.\n",
    "    \"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_ckpt):\n",
    "    \"\"\"\"Training the model after tokenizing the batched dataset.\n",
    "    Args:\n",
    "        model_ckpt: Model Checkpoint\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_ckpt, label2id=config[\"label2id\"], id2label=config[\"id2label\"])\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_ckpt, config=config).to(device)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while initializing model {e}\")\n",
    "\n",
    "    def local_tokenizer(batch):\n",
    "        \"\"\"Tokenize the given batch\"\"\"\n",
    "        try:\n",
    "            temp = tokenizer(batch[\"title\"], padding=True, truncation=True)\n",
    "            return temp\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error while tokenize: {e}\")\n",
    "\n",
    "    encoded_dataset = dataset.map(\n",
    "        local_tokenizer, batched=True, batch_size=None)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=config[\"output_path\"],\n",
    "        overwrite_output_dir=True,\n",
    "        #   num_train_epochs = 2,\n",
    "        max_steps=50,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=config[\"batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"batch_size\"],\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        report_to=\"none\")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=encoded_dataset[\"train\"],\n",
    "        eval_dataset=encoded_dataset[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_args\n",
    "    )\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error while training {e}\")\n",
    "\n",
    "    preds = trainer.predict(encoded_dataset[\"test\"])\n",
    "\n",
    "    return preds.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(config[\"model_checkpoint\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
